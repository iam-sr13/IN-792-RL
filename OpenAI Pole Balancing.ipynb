{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Simulation in OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||Action-Space|| = 2\n",
      "||State-Space|| range: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        self.env_discrete = type(env.observation_space) == gym.spaces.discrete.Discrete\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "            print(\"||Action-Space|| = {}\".format(self.action_size))\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            print(\"||Action-Space|| range:\", self.action_low, self.action_high)\n",
    "            \n",
    "        if self.env_discrete:\n",
    "            self.state_size = env.observation_space.n\n",
    "            print(\"||State-Space|| = {}\".format(self.state_size))\n",
    "        else:\n",
    "            self.state_low = env.observation_space.low\n",
    "            self.state_high = env.observation_space.high\n",
    "            self.state_shape = env.observation_space.shape\n",
    "            print(\"||State-Space|| range:\", self.state_low, self.state_high)      \n",
    "        \n",
    "        self.states = env.observation_space\n",
    "        self.actions = env.action_space        \n",
    "    \n",
    "    def get_action_random(self, state):\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
    "        return action\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        pole_angle = state[2]\n",
    "        action = 0 if pole_angle < 0 else 1\n",
    "        return action\n",
    "    \n",
    "myagent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the no. of episodes for simulation: 5\n",
      "In State:  [-0.02678381  0.01326885  0.04522186 -0.03733366]\n",
      "In State:  [-0.02651843  0.20771412  0.04447518 -0.31541269]\n",
      "In State:  [-0.02236415  0.40217526  0.03816693 -0.59374457]\n",
      "In State:  [-0.01432064  0.59674277  0.02629204 -0.87416493]\n",
      "In State:  [-0.00238579  0.79149758  0.00880874 -1.15846721]\n",
      "In State:  [ 0.01344416  0.98650363 -0.0143606  -1.4483752 ]\n",
      "In State:  [ 0.03317424  0.79156116 -0.04332811 -1.16021351]\n",
      "In State:  [ 0.04900546  0.59702964 -0.06653238 -0.88142457]\n",
      "In State:  [ 0.06094605  0.40287145 -0.08416087 -0.61037773]\n",
      "In State:  [ 0.06900348  0.20902047 -0.09636842 -0.34534455]\n",
      "In State:  [ 0.07318389  0.01539202 -0.10327532 -0.08453796]\n",
      "In State:  [ 0.07349173 -0.17810957 -0.10496607  0.17385891]\n",
      "In State:  [ 0.06992954 -0.37158487 -0.1014889   0.43167118]\n",
      "In State:  [ 0.06249784 -0.56513434 -0.09285547  0.69071496]\n",
      "In State:  [ 0.05119515 -0.75885355 -0.07904117  0.95278166]\n",
      "In State:  [ 0.03601808 -0.95282809 -0.05998554  1.21962114]\n",
      "In State:  [ 0.01696152 -1.14712772 -0.03559312  1.49292122]\n",
      "In State:  [-0.00598103 -1.34179899 -0.00573469  1.77428101]\n",
      "In State:  [-0.03281701 -1.53685584  0.02975093  2.06517548]\n",
      "In State:  [-0.06355413 -1.34204915  0.07105444  1.78184024]\n",
      "In State:  [-0.09039511 -1.14779453  0.10669124  1.51206599]\n",
      "In State:  [-0.113351   -0.95411428  0.13693256  1.25450529]\n",
      "In State:  [-0.13243329 -0.76098541  0.16202267  1.00765618]\n",
      "In State:  [-0.147653   -0.56835364  0.18217579  0.76991939]\n",
      "In State:  [-0.15902007 -0.37614378  0.19757418  0.53963959]\n",
      "In State:  [-0.16654295 -0.18426735  0.20836697  0.31513384]\n",
      "Episode 1 done in 26 timesteps!\n",
      "\n",
      "In State:  [ 0.04976561 -0.02957156  0.00855955  0.03842785]\n",
      "In State:  [ 0.04917417  0.16542661  0.0093281  -0.25154221]\n",
      "In State:  [ 0.05248271  0.36041413  0.00429726 -0.54126833]\n",
      "In State:  [ 0.05969099  0.55547541 -0.00652811 -0.83259417]\n",
      "In State:  [ 0.0708005   0.36044328 -0.02317999 -0.54197145]\n",
      "In State:  [ 0.07800936  0.16565466 -0.03401942 -0.25668132]\n",
      "In State:  [ 0.08132246 -0.0289655  -0.03915305  0.02508022]\n",
      "In State:  [ 0.08074315 -0.22350472 -0.03865144  0.30515737]\n",
      "In State:  [ 0.07627305 -0.41805516 -0.03254829  0.58540437]\n",
      "In State:  [ 0.06791195 -0.61270645 -0.02084021  0.86765915]\n",
      "In State:  [ 0.05565782 -0.80753872 -0.00348702  1.15371752]\n",
      "In State:  [ 0.03950704 -1.00261502  0.01958733  1.44530501]\n",
      "In State:  [ 0.01945474 -0.80773946  0.04849343  1.15880605]\n",
      "In State:  [ 0.00329995 -0.61328182  0.07166955  0.88171396]\n",
      "In State:  [-0.00896568 -0.4192027   0.08930383  0.61239545]\n",
      "In State:  [-0.01734974 -0.22543484  0.10155174  0.34912233]\n",
      "In State:  [-0.02185843 -0.03189279  0.10853418  0.09010985]\n",
      "In State:  [-0.02249629  0.16151962  0.11033638 -0.16645526]\n",
      "In State:  [-0.0192659   0.35490341  0.10700727 -0.42239391]\n",
      "In State:  [-0.01216783  0.54835953  0.0985594  -0.67951815]\n",
      "In State:  [-0.00120064  0.74198428  0.08496903 -0.9396162 ]\n",
      "In State:  [ 0.01363905  0.93586443  0.06617671 -1.20443639]\n",
      "In State:  [ 0.03235634  1.13007153  0.04208798 -1.47566764]\n",
      "In State:  [ 0.05495777  1.32465478  0.01257463 -1.75491381]\n",
      "In State:  [ 0.08145086  1.51963193 -0.02252365 -2.04365958]\n",
      "In State:  [ 0.1118435   1.32474836 -0.06339684 -1.75802976]\n",
      "In State:  [ 0.13833847  1.13039922 -0.09855743 -1.48571789]\n",
      "In State:  [ 0.16094645  0.93660698 -0.12827179 -1.2253692 ]\n",
      "In State:  [ 0.17967859  0.74334853 -0.15277918 -0.97547168]\n",
      "In State:  [ 0.19454556  0.55056914 -0.17228861 -0.73441339]\n",
      "In State:  [ 0.20555695  0.35819295 -0.18697688 -0.50052394]\n",
      "In State:  [ 0.21272081  0.16613064 -0.19698736 -0.2721032 ]\n",
      "In State:  [ 0.21604342 -0.02571518 -0.20242942 -0.04744019]\n",
      "In State:  [ 0.21552912 -0.21744603 -0.20337822  0.17517504]\n",
      "In State:  [ 0.21118019 -0.40916477 -0.19987472  0.39744427]\n",
      "In State:  [ 0.2029969  -0.60097329 -0.19192584  0.62105643]\n",
      "In State:  [ 0.19097743 -0.79297046 -0.17950471  0.84768379]\n",
      "In State:  [ 0.17511802 -0.98524967 -0.16255103  1.07897694]\n",
      "In State:  [ 0.15541303 -1.17789588 -0.1409715   1.31655712]\n",
      "In State:  [ 0.13185511 -1.37098146 -0.11464035  1.56200389]\n",
      "In State:  [ 0.10443548 -1.56456061 -0.08340028  1.81683615]\n",
      "In State:  [ 0.07314427 -1.75866173 -0.04706355  2.08248458]\n",
      "In State:  [ 0.03797104 -1.95327725 -0.00541386  2.36025304]\n",
      "In State:  [-1.09450777e-03 -2.14835057e+00  4.17912002e-02  2.65126706e+00]\n",
      "In State:  [-0.04406152 -1.95356563  0.09481654  2.3716278 ]\n",
      "In State:  [-0.08313283 -1.75940317  0.1422491   2.10952661]\n",
      "In State:  [-0.1183209  -1.56596285  0.18443963  1.86397719]\n",
      "Episode 2 done in 47 timesteps!\n",
      "\n",
      "In State:  [ 0.02436579  0.01474098 -0.01321399 -0.0207615 ]\n",
      "In State:  [ 0.02466061 -0.180189   -0.01362922  0.26772315]\n",
      "In State:  [ 0.02105683 -0.37511382 -0.00827476  0.55607635]\n",
      "In State:  [ 0.01355455 -0.57011862  0.00284677  0.84614079]\n",
      "In State:  [ 0.00215218 -0.37503562  0.01976959  0.55435443]\n",
      "In State:  [-0.00534853 -0.18019677  0.03085668  0.26796514]\n",
      "In State:  [-0.00895247  0.01447154  0.03621598 -0.01482789]\n",
      "In State:  [-0.00866304  0.20905589  0.03591942 -0.29586787]\n",
      "In State:  [-0.00448192  0.40364785  0.03000206 -0.57700949]\n",
      "In State:  [ 0.00359104  0.59833672  0.01846187 -0.86009208]\n",
      "In State:  [ 0.01555777  0.79320242  0.00126003 -1.14691334]\n",
      "In State:  [ 0.03142182  0.9883079  -0.02167823 -1.43920087]\n",
      "In State:  [ 0.05118798  0.79345964 -0.05046225 -1.15337006]\n",
      "In State:  [ 0.06705717  0.5990309  -0.07352965 -0.8769278 ]\n",
      "In State:  [ 0.07903779  0.40498119 -0.09106821 -0.60823799]\n",
      "In State:  [ 0.08713741  0.2112425  -0.10323297 -0.34557125]\n",
      "In State:  [ 0.09136226  0.01772892 -0.11014439 -0.08714283]\n",
      "In State:  [ 0.09171684 -0.17565599 -0.11188725  0.16885971]\n",
      "In State:  [ 0.08820372 -0.3690134  -0.10851006  0.42425601]\n",
      "In State:  [ 0.08082345 -0.56244436 -0.10002494  0.68085659]\n",
      "In State:  [ 0.06957456 -0.75604526 -0.0864078   0.9404481 ]\n",
      "In State:  [ 0.05445366 -0.94990298 -0.06759884  1.20477751]\n",
      "In State:  [ 0.0354556  -1.14408924 -0.04350329  1.47553271]\n",
      "In State:  [ 0.01257381 -1.33865353 -0.01399264  1.75431709]\n",
      "In State:  [-0.01419926 -1.53361405  0.0210937   2.04261554]\n",
      "In State:  [-0.04487154 -1.33871501  0.06194602  1.75653311]\n",
      "In State:  [-0.07164584 -1.14434734  0.09707668  1.4837413 ]\n",
      "In State:  [-0.09453278 -0.95053394  0.1267515   1.2228857 ]\n",
      "In State:  [-0.11354346 -0.75725195  0.15120922  0.97245378]\n",
      "In State:  [-0.1286885  -0.56444679  0.17065829  0.7308323 ]\n",
      "In State:  [-0.13997744 -0.37204264  0.18527494  0.49634896]\n",
      "In State:  [-0.14741829 -0.17995012  0.19520192  0.26730123]\n",
      "In State:  [-0.15101729  0.01192823  0.20054794  0.04197538]\n",
      "In State:  [-0.15077873  0.20369405  0.20138745 -0.18134152]\n",
      "In State:  [-0.14670485  0.39545029  0.19776062 -0.40435428]\n",
      "In State:  [-0.13879584  0.58729889  0.18967353 -0.62875482]\n",
      "In State:  [-0.12704986  0.77933865  0.17709844 -0.85621808]\n",
      "In State:  [-0.11146309  0.97166275  0.15997408 -1.08839684]\n",
      "In State:  [-0.09202984  1.16435572  0.13820614 -1.32691364]\n",
      "In State:  [-0.06874272  1.35748917  0.11166787 -1.57334807]\n",
      "In State:  [-0.04159294  1.55111611  0.08020091 -1.82921734]\n",
      "In State:  [-0.01057062  1.74526314  0.04361656 -2.09594799]\n",
      "In State:  [ 2.43346474e-02  1.93992015e+00  1.69759893e-03 -2.37483662e+00]\n",
      "In State:  [ 0.06313305  2.13502706 -0.04579913 -2.66699746]\n",
      "In State:  [ 0.10583359  1.94027318 -0.09913908 -2.38863321]\n",
      "In State:  [ 0.14463906  1.74615193 -0.14691175 -2.12798028]\n",
      "In State:  [ 0.17956209  1.55276324 -0.18947135 -1.88405891]\n",
      "Episode 3 done in 47 timesteps!\n",
      "\n",
      "In State:  [-0.02383806 -0.00309212  0.04979713  0.02284689]\n",
      "In State:  [-0.0238999   0.19128163  0.05025406 -0.25371801]\n",
      "In State:  [-0.02007427  0.38565134  0.0451797  -0.53013603]\n",
      "In State:  [-0.01236124  0.58010959  0.03457698 -0.80824744]\n",
      "In State:  [-7.59046404e-04  7.74741068e-01  1.84120345e-02 -1.08985654e+00]\n",
      "In State:  [ 0.01473577  0.96961552 -0.0033851  -1.37670585]\n",
      "In State:  [ 0.03412809  0.77453602 -0.03091921 -1.08508349]\n",
      "In State:  [ 0.04961881  0.57983532 -0.05262088 -0.80226083]\n",
      "In State:  [ 0.06121551  0.38547296 -0.0686661  -0.52658422]\n",
      "In State:  [ 0.06892497  0.19138098 -0.07919778 -0.25630432]\n",
      "In State:  [ 0.07275259 -0.00252617 -0.08432387  0.01038488]\n",
      "In State:  [ 0.07270207 -0.19634392 -0.08411617  0.27531667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In State:  [ 0.06877519 -0.39017131 -0.07860984  0.54032877]\n",
      "In State:  [ 0.06097176 -0.58410538 -0.06780326  0.80724403]\n",
      "In State:  [ 0.04928966 -0.77823578 -0.05165838  1.07785164]\n",
      "In State:  [ 0.03372494 -0.97263879 -0.03010135  1.35388635]\n",
      "In State:  [ 0.01427216 -1.16737022 -0.00302362  1.63700271]\n",
      "In State:  [-0.00907524 -1.36245658  0.02971643  1.92874197]\n",
      "In State:  [-0.03632437 -1.1676653   0.06829127  1.6454194 ]\n",
      "In State:  [-0.05967768 -0.97340565  0.10119966  1.37477117]\n",
      "In State:  [-0.07914579 -0.77968355  0.12869508  1.11537668]\n",
      "In State:  [-0.09473946 -0.58646407  0.15100261  0.86567627]\n",
      "In State:  [-0.10646874 -0.39368417  0.16831614  0.6240232 ]\n",
      "In State:  [-0.11434243 -0.20126227  0.1807966   0.38872086]\n",
      "In State:  [-0.11836767 -0.00910517  0.18857102  0.15804833]\n",
      "In State:  [-0.11854978  0.18288698  0.19173199 -0.06972285]\n",
      "In State:  [-0.11489204  0.37481704  0.19033753 -0.29631799]\n",
      "In State:  [-0.10739569  0.56678816  0.18441117 -0.52345234]\n",
      "In State:  [-0.09605993  0.75890131  0.17394212 -0.75282585]\n",
      "In State:  [-0.08088191  0.95125282  0.15888561 -0.9861178 ]\n",
      "In State:  [-0.06185685  1.14393145  0.13916325 -1.22497926]\n",
      "In State:  [-0.03897822  1.3370145   0.11466367 -1.47102179]\n",
      "In State:  [-0.01223793  1.53056253  0.08524323 -1.7258001 ]\n",
      "In State:  [ 0.01837332  1.72461236  0.05072723 -1.99078677]\n",
      "In State:  [ 0.05286557  1.91916751  0.01091149 -2.26733669]\n",
      "In State:  [ 0.09124892  2.11418601 -0.03443524 -2.55663911]\n",
      "In State:  [ 0.13353264  1.91935487 -0.08556802 -2.27468761]\n",
      "In State:  [ 0.17191974  1.72512649 -0.13106178 -2.00953729]\n",
      "In State:  [ 0.20642227  1.53158969 -0.17125252 -1.76014379]\n",
      "In State:  [ 0.23705406  1.33877067 -0.2064554  -1.52524856]\n",
      "Episode 4 done in 40 timesteps!\n",
      "\n",
      "In State:  [ 0.01577043 -0.02856668  0.04175768  0.0425118 ]\n",
      "In State:  [ 0.01519909  0.16593237  0.04260792 -0.23670927]\n",
      "In State:  [ 0.01851774  0.36042051  0.03787373 -0.51565377]\n",
      "In State:  [ 0.02572615  0.55498923  0.02756066 -0.79616533]\n",
      "In State:  [ 0.03682594  0.74972234  0.01163735 -1.08005226]\n",
      "In State:  [ 0.05182038  0.94468872 -0.00996369 -1.36906073]\n",
      "In State:  [ 0.07071416  0.74969285 -0.03734491 -1.07951072]\n",
      "In State:  [ 0.08570801  0.55508339 -0.05893512 -0.7987769 ]\n",
      "In State:  [ 0.09680968  0.36081735 -0.07491066 -0.52520066]\n",
      "In State:  [ 0.10402603  0.16682507 -0.08541467 -0.25703147]\n",
      "In State:  [ 0.10736253 -0.02698015 -0.0905553   0.00753517]\n",
      "In State:  [ 0.10682293 -0.22069458 -0.0904046   0.27032936]\n",
      "In State:  [ 0.10240904 -0.41441795 -0.08499801  0.53318499]\n",
      "In State:  [ 0.09412068 -0.6082481  -0.07433431  0.79792123]\n",
      "In State:  [ 0.08195571 -0.80227574 -0.05837589  1.06632481]\n",
      "In State:  [ 0.0659102  -0.99657872 -0.03704939  1.34013006]\n",
      "In State:  [ 0.04597863 -1.19121517 -0.01024679  1.62099436]\n",
      "In State:  [ 0.02215432 -1.38621495  0.0221731   1.91046616]\n",
      "In State:  [-0.00556998 -1.19133895  0.06038242  1.62474237]\n",
      "In State:  [-0.02939676 -0.99697724  0.09287727  1.35147278]\n",
      "In State:  [-0.0493363  -0.80313638  0.11990672  1.08923134]\n",
      "In State:  [-0.06539903 -0.60978161  0.14169135  0.83644984]\n",
      "In State:  [-0.07759466 -0.41684976  0.15842035  0.59147026]\n",
      "In State:  [-0.08593166 -0.22425885  0.17024975  0.35258241]\n",
      "In State:  [-0.09041683 -0.03191507  0.1773014   0.11804995]\n",
      "In State:  [-0.09105513  0.16028199  0.1796624  -0.11387218]\n",
      "In State:  [-0.08784949  0.3524357   0.17738496 -0.34492635]\n",
      "In State:  [-0.08080078  0.54464928  0.17048643 -0.57684443]\n",
      "In State:  [-0.06990779  0.7370232   0.15894954 -0.81134131]\n",
      "In State:  [-0.05516733  0.92965232  0.14272271 -1.05010796]\n",
      "In State:  [-0.03657428  1.12262246  0.12172055 -1.29480193]\n",
      "In State:  [-0.01412183  1.31600594  0.09582452 -1.54703339]\n",
      "In State:  [ 0.01219828  1.50985558  0.06488385 -1.80834457]\n",
      "In State:  [ 0.0423954   1.70419669  0.02871696 -2.08018036]\n",
      "In State:  [ 0.07647933  1.89901648 -0.01288665 -2.36384794]\n",
      "In State:  [ 0.11445966  1.70401145 -0.06016361 -2.07515324]\n",
      "In State:  [ 0.14853989  1.50954908 -0.10166667 -1.80166488]\n",
      "In State:  [ 0.17873087  1.31569985 -0.13769997 -1.54223101]\n",
      "In State:  [ 0.20504487  1.12247572 -0.16854459 -1.29549429]\n",
      "In State:  [ 0.22749438  0.92984738 -0.19445448 -1.05996394]\n",
      "Episode 5 done in 40 timesteps!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"Enter the no. of episodes for simulation: \"))\n",
    "\n",
    "for episode in range(n):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):\n",
    "        env.render()\n",
    "        print(\"In State: \", state)\n",
    "        action = myagent.get_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode {} done in {} timesteps!\\n\".format(episode+1, t+1))\n",
    "            break\n",
    "    else:\n",
    "        print(\"Episode {} NOT done in limit of {} timesteps! Agent Terminated!\\n\".format(episode+1, t+1))\n",
    "    \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||Action-Space|| = 2\n",
      "||State-Space|| range: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "QTable constructed with size: (1, 1, 6, 12, 2)\n"
     ]
    }
   ],
   "source": [
    "class QLAgent(Agent):\n",
    "    def __init__(self, env, discount_rate=0.97, learning_rate=0.01):\n",
    "        super().__init__(env)        \n",
    "        \n",
    "        self.eps = 1.0\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.state_size = (1, 1, 6, 12,)\n",
    "        self.build_model()\n",
    "        print(\"QTable constructed with size: {}\".format(self.q_table.shape))\n",
    "        \n",
    "    def discretize(self, state):\n",
    "        upper_bounds = [self.state_high[0], 0.5, self.state_high[2], math.radians(50)]\n",
    "        lower_bounds = [self.state_low[0], -0.5, self.state_low[2], -math.radians(50)]\n",
    "        ratios = [(state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(state))]\n",
    "        new_obs = [int(round((self.state_size[i] - 1) * ratios[i])) for i in range(len(state))]\n",
    "        new_obs = [min(self.state_size[i] - 1, max(0, new_obs[i])) for i in range(len(state))]\n",
    "        return tuple(new_obs)\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.q_table = 1e-4*np.random.random(self.state_size + (self.action_size,))\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = self.discretize(state)\n",
    "        q_state = self.q_table[state]\n",
    "        action_greedy = np.argmax(q_state)\n",
    "        action_random = super().get_action_random(state)\n",
    "        return action_random if random.random() < self.eps else action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        state = self.discretize(state)\n",
    "        next_state = self.discretize(next_state)\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        q_next = np.zeros([self.action_size]) if done else q_next\n",
    "        q_target = reward + self.discount_rate * np.max(q_next)\n",
    "        #print((*state,action))\n",
    "        q_update = q_target - self.q_table[(*state,action)]\n",
    "        self.q_table[(*state,action)] += self.learning_rate * q_update\n",
    "        \n",
    "        if done:\n",
    "            self.eps = self.eps * 0.99\n",
    "        \n",
    "myagent = QLAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the no. of episodes for simulation: 100\n",
      "Episode 1 done in 11 timesteps! Total reward: 11.0, eps: 0.99\n",
      "\n",
      "Episode 2 done in 14 timesteps! Total reward: 25.0, eps: 0.9801\n",
      "\n",
      "Episode 3 done in 14 timesteps! Total reward: 39.0, eps: 0.9702989999999999\n",
      "\n",
      "Episode 4 done in 68 timesteps! Total reward: 107.0, eps: 0.96059601\n",
      "\n",
      "Episode 5 done in 28 timesteps! Total reward: 135.0, eps: 0.9509900498999999\n",
      "\n",
      "Episode 6 done in 18 timesteps! Total reward: 153.0, eps: 0.9414801494009999\n",
      "\n",
      "Episode 7 done in 18 timesteps! Total reward: 171.0, eps: 0.9320653479069899\n",
      "\n",
      "Episode 8 done in 58 timesteps! Total reward: 229.0, eps: 0.92274469442792\n",
      "\n",
      "Episode 9 done in 11 timesteps! Total reward: 240.0, eps: 0.9135172474836407\n",
      "\n",
      "Episode 10 done in 20 timesteps! Total reward: 260.0, eps: 0.9043820750088043\n",
      "\n",
      "Episode 11 done in 22 timesteps! Total reward: 282.0, eps: 0.8953382542587163\n",
      "\n",
      "Episode 12 done in 13 timesteps! Total reward: 295.0, eps: 0.8863848717161291\n",
      "\n",
      "Episode 13 done in 17 timesteps! Total reward: 312.0, eps: 0.8775210229989678\n",
      "\n",
      "Episode 14 done in 23 timesteps! Total reward: 335.0, eps: 0.8687458127689781\n",
      "\n",
      "Episode 15 done in 39 timesteps! Total reward: 374.0, eps: 0.8600583546412883\n",
      "\n",
      "Episode 16 done in 19 timesteps! Total reward: 393.0, eps: 0.8514577710948754\n",
      "\n",
      "Episode 17 done in 13 timesteps! Total reward: 406.0, eps: 0.8429431933839266\n",
      "\n",
      "Episode 18 done in 12 timesteps! Total reward: 418.0, eps: 0.8345137614500874\n",
      "\n",
      "Episode 19 done in 16 timesteps! Total reward: 434.0, eps: 0.8261686238355865\n",
      "\n",
      "Episode 20 done in 24 timesteps! Total reward: 458.0, eps: 0.8179069375972307\n",
      "\n",
      "Episode 21 done in 11 timesteps! Total reward: 469.0, eps: 0.8097278682212583\n",
      "\n",
      "Episode 22 done in 23 timesteps! Total reward: 492.0, eps: 0.8016305895390458\n",
      "\n",
      "Episode 23 done in 15 timesteps! Total reward: 507.0, eps: 0.7936142836436553\n",
      "\n",
      "Episode 24 done in 15 timesteps! Total reward: 522.0, eps: 0.7856781408072188\n",
      "\n",
      "Episode 25 done in 36 timesteps! Total reward: 558.0, eps: 0.7778213593991465\n",
      "\n",
      "Episode 26 done in 16 timesteps! Total reward: 574.0, eps: 0.7700431458051551\n",
      "\n",
      "Episode 27 done in 38 timesteps! Total reward: 612.0, eps: 0.7623427143471035\n",
      "\n",
      "Episode 28 done in 13 timesteps! Total reward: 625.0, eps: 0.7547192872036325\n",
      "\n",
      "Episode 29 done in 10 timesteps! Total reward: 635.0, eps: 0.7471720943315961\n",
      "\n",
      "Episode 30 done in 9 timesteps! Total reward: 644.0, eps: 0.7397003733882802\n",
      "\n",
      "Episode 31 done in 23 timesteps! Total reward: 667.0, eps: 0.7323033696543974\n",
      "\n",
      "Episode 32 done in 31 timesteps! Total reward: 698.0, eps: 0.7249803359578534\n",
      "\n",
      "Episode 33 done in 18 timesteps! Total reward: 716.0, eps: 0.7177305325982748\n",
      "\n",
      "Episode 34 done in 10 timesteps! Total reward: 726.0, eps: 0.7105532272722921\n",
      "\n",
      "Episode 35 done in 10 timesteps! Total reward: 736.0, eps: 0.7034476949995692\n",
      "\n",
      "Episode 36 done in 26 timesteps! Total reward: 762.0, eps: 0.6964132180495735\n",
      "\n",
      "Episode 37 done in 44 timesteps! Total reward: 806.0, eps: 0.6894490858690777\n",
      "\n",
      "Episode 38 done in 42 timesteps! Total reward: 848.0, eps: 0.682554595010387\n",
      "\n",
      "Episode 39 done in 48 timesteps! Total reward: 896.0, eps: 0.6757290490602831\n",
      "\n",
      "Episode 40 done in 24 timesteps! Total reward: 920.0, eps: 0.6689717585696803\n",
      "\n",
      "Episode 41 done in 43 timesteps! Total reward: 963.0, eps: 0.6622820409839835\n",
      "\n",
      "Episode 42 done in 10 timesteps! Total reward: 973.0, eps: 0.6556592205741436\n",
      "\n",
      "Episode 43 done in 13 timesteps! Total reward: 986.0, eps: 0.6491026283684022\n",
      "\n",
      "Episode 44 done in 18 timesteps! Total reward: 1004.0, eps: 0.6426116020847181\n",
      "\n",
      "Episode 45 done in 29 timesteps! Total reward: 1033.0, eps: 0.6361854860638709\n",
      "\n",
      "Episode 46 done in 13 timesteps! Total reward: 1046.0, eps: 0.6298236312032323\n",
      "\n",
      "Episode 47 done in 18 timesteps! Total reward: 1064.0, eps: 0.6235253948912\n",
      "\n",
      "Episode 48 done in 14 timesteps! Total reward: 1078.0, eps: 0.617290140942288\n",
      "\n",
      "Episode 49 done in 19 timesteps! Total reward: 1097.0, eps: 0.6111172395328651\n",
      "\n",
      "Episode 50 done in 27 timesteps! Total reward: 1124.0, eps: 0.6050060671375365\n",
      "\n",
      "Episode 51 done in 14 timesteps! Total reward: 1138.0, eps: 0.5989560064661611\n",
      "\n",
      "Episode 52 done in 11 timesteps! Total reward: 1149.0, eps: 0.5929664464014994\n",
      "\n",
      "Episode 53 done in 14 timesteps! Total reward: 1163.0, eps: 0.5870367819374844\n",
      "\n",
      "Episode 54 done in 17 timesteps! Total reward: 1180.0, eps: 0.5811664141181095\n",
      "\n",
      "Episode 55 done in 14 timesteps! Total reward: 1194.0, eps: 0.5753547499769285\n",
      "\n",
      "Episode 56 done in 26 timesteps! Total reward: 1220.0, eps: 0.5696012024771592\n",
      "\n",
      "Episode 57 done in 67 timesteps! Total reward: 1287.0, eps: 0.5639051904523876\n",
      "\n",
      "Episode 58 done in 14 timesteps! Total reward: 1301.0, eps: 0.5582661385478638\n",
      "\n",
      "Episode 59 done in 11 timesteps! Total reward: 1312.0, eps: 0.5526834771623851\n",
      "\n",
      "Episode 60 done in 21 timesteps! Total reward: 1333.0, eps: 0.5471566423907612\n",
      "\n",
      "Episode 61 done in 26 timesteps! Total reward: 1359.0, eps: 0.5416850759668536\n",
      "\n",
      "Episode 62 done in 16 timesteps! Total reward: 1375.0, eps: 0.536268225207185\n",
      "\n",
      "Episode 63 done in 14 timesteps! Total reward: 1389.0, eps: 0.5309055429551132\n",
      "\n",
      "Episode 64 done in 27 timesteps! Total reward: 1416.0, eps: 0.525596487525562\n",
      "\n",
      "Episode 65 done in 22 timesteps! Total reward: 1438.0, eps: 0.5203405226503064\n",
      "\n",
      "Episode 66 done in 23 timesteps! Total reward: 1461.0, eps: 0.5151371174238033\n",
      "\n",
      "Episode 67 done in 19 timesteps! Total reward: 1480.0, eps: 0.5099857462495653\n",
      "\n",
      "Episode 68 done in 14 timesteps! Total reward: 1494.0, eps: 0.5048858887870696\n",
      "\n",
      "Episode 69 done in 23 timesteps! Total reward: 1517.0, eps: 0.4998370298991989\n",
      "\n",
      "Episode 70 done in 16 timesteps! Total reward: 1533.0, eps: 0.49483865960020695\n",
      "\n",
      "Episode 71 done in 15 timesteps! Total reward: 1548.0, eps: 0.4898902730042049\n",
      "\n",
      "Episode 72 done in 27 timesteps! Total reward: 1575.0, eps: 0.48499137027416284\n",
      "\n",
      "Episode 73 done in 17 timesteps! Total reward: 1592.0, eps: 0.4801414565714212\n",
      "\n",
      "Episode 74 done in 30 timesteps! Total reward: 1622.0, eps: 0.475340042005707\n",
      "\n",
      "Episode 75 done in 25 timesteps! Total reward: 1647.0, eps: 0.47058664158564995\n",
      "\n",
      "Episode 76 done in 19 timesteps! Total reward: 1666.0, eps: 0.4658807751697934\n",
      "\n",
      "Episode 77 done in 9 timesteps! Total reward: 1675.0, eps: 0.4612219674180955\n",
      "\n",
      "Episode 78 done in 14 timesteps! Total reward: 1689.0, eps: 0.45660974774391455\n",
      "\n",
      "Episode 79 done in 16 timesteps! Total reward: 1705.0, eps: 0.4520436502664754\n",
      "\n",
      "Episode 80 done in 16 timesteps! Total reward: 1721.0, eps: 0.44752321376381066\n",
      "\n",
      "Episode 81 done in 13 timesteps! Total reward: 1734.0, eps: 0.44304798162617254\n",
      "\n",
      "Episode 82 done in 25 timesteps! Total reward: 1759.0, eps: 0.4386175018099108\n",
      "\n",
      "Episode 83 done in 14 timesteps! Total reward: 1773.0, eps: 0.4342313267918117\n",
      "\n",
      "Episode 84 done in 26 timesteps! Total reward: 1799.0, eps: 0.4298890135238936\n",
      "\n",
      "Episode 85 done in 12 timesteps! Total reward: 1811.0, eps: 0.42559012338865465\n",
      "\n",
      "Episode 86 done in 16 timesteps! Total reward: 1827.0, eps: 0.4213342221547681\n",
      "\n",
      "Episode 87 done in 10 timesteps! Total reward: 1837.0, eps: 0.41712087993322045\n",
      "\n",
      "Episode 88 done in 76 timesteps! Total reward: 1913.0, eps: 0.41294967113388825\n",
      "\n",
      "Episode 89 done in 15 timesteps! Total reward: 1928.0, eps: 0.40882017442254937\n",
      "\n",
      "Episode 90 done in 18 timesteps! Total reward: 1946.0, eps: 0.4047319726783239\n",
      "\n",
      "Episode 91 done in 16 timesteps! Total reward: 1962.0, eps: 0.40068465295154065\n",
      "\n",
      "Episode 92 done in 19 timesteps! Total reward: 1981.0, eps: 0.39667780642202527\n",
      "\n",
      "Episode 93 done in 23 timesteps! Total reward: 2004.0, eps: 0.392711028357805\n",
      "\n",
      "Episode 94 done in 69 timesteps! Total reward: 2073.0, eps: 0.38878391807422696\n",
      "\n",
      "Episode 95 done in 27 timesteps! Total reward: 2100.0, eps: 0.3848960788934847\n",
      "\n",
      "Episode 96 done in 14 timesteps! Total reward: 2114.0, eps: 0.38104711810454983\n",
      "\n",
      "Episode 97 done in 23 timesteps! Total reward: 2137.0, eps: 0.37723664692350434\n",
      "\n",
      "Episode 98 done in 16 timesteps! Total reward: 2153.0, eps: 0.37346428045426927\n",
      "\n",
      "Episode 99 done in 22 timesteps! Total reward: 2175.0, eps: 0.36972963764972655\n",
      "\n",
      "Episode 100 done in 13 timesteps! Total reward: 2188.0, eps: 0.36603234127322926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"Enter the no. of episodes for simulation: \"))\n",
    "\n",
    "total_reward = 0\n",
    "for ep in range(n):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):\n",
    "        env.render()        \n",
    "        action = myagent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)     \n",
    "        \n",
    "        myagent.train((state,action,next_state,reward,done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        #print(\"In State: \", state)   \n",
    "        #print(\"In State (Discrete): \", myagent.discretize(state), \"Action: \", action)       \n",
    "        #print(\"Episode: {}, Total reward: {}, eps: {}\".format(ep,total_reward,myagent.eps))\n",
    "        #print(myagent.q_table)\n",
    "        #time.sleep(0.05)\n",
    "        #clear_output(wait=True)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode {} done in {} timesteps! Total reward: {}, eps: {}\\n\".format(ep+1, t+1, total_reward, myagent.eps))\n",
    "            break\n",
    "    else:\n",
    "        print(\"Episode {} NOT done in limit of {} timesteps! Agent Terminated!\\n\".format(ep+1, t+1))  \n",
    "    \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myagent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
